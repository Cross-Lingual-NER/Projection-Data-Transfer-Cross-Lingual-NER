"""This module implements intrinsic evaluation strategy of pipelines
by comparing labels which are generated by a pipeline and original
ground truth labels for the given dataset
"""

import datasets
import evaluate
import mlflow

from src.pipelines.transforms_base import PipelineTransformBase


class EvaluateByCompareTransform(PipelineTransformBase):
    """Given a NER dataset we can evaluate our pipeline by computing
    seqeval score for the original and generated labels. This utility performs
    it and optionally writes metrics to MLFlow.
    """

    is_blocking = True

    def __init__(
        self,
        orig_label_key: str = "ner_tags",
        gen_label_key: str = "ner_tags",
        log_to_mlflow: bool = True,
        orig_split: str | None = None,
        labels_to_ignore: list[str] | None = None,
    ) -> None:
        super().__init__()
        self._orig_key = orig_label_key
        self._gen_key = gen_label_key
        self._log_mflow = log_to_mlflow
        self._orig_split = orig_split
        self._labels_to_ignore = labels_to_ignore

    @staticmethod
    def _count_entities(labels: list[str]) -> int:
        return sum(label.startswith("B-") for label in labels)

    def _log_to_mlflow(
        self, metrics_results: dict[str, dict[str, float] | float]
    ) -> None:
        for gkey, gvalue in metrics_results.items():
            if isinstance(gvalue, dict):
                for key, value in gvalue.items():
                    mlflow.log_metric(f"{gkey}_{key}", float(value))
            else:
                mlflow.log_metric(gkey, float(gvalue))

    def _prepate_tags_to_label_map(self, ds_orig: datasets.Dataset) -> list[str]:
        label_list = ds_orig.features["ner_tags"].feature.names
        # Make all to be ignored labels equal to O
        if self._labels_to_ignore is not None:
            label_list = [
                label if label not in self._labels_to_ignore else "O"
                for label in label_list
            ]
        return label_list

    def __call__(self, input: tuple[datasets.Dataset, datasets.Dataset]) -> None:
        ds_orig, ds_gen = input

        if self._orig_split is not None:
            ds_orig = ds_orig[self._orig_split]

        label_list = self._prepate_tags_to_label_map(ds_orig)

        metric = evaluate.load("seqeval")

        BATCH_SIZE = 1024
        N_rows = len(ds_gen)
        n_it = 0
        proj_ratio_acc = 0
        while True:
            r_start = n_it * BATCH_SIZE
            r_end = min(N_rows, (n_it + 1) * BATCH_SIZE)

            gen_labels = ds_gen.select(range(r_start, r_end))[self._gen_key]
            gt_labels = ds_orig.select(range(r_start, r_end))[self._orig_key]

            predictions = [
                [label_list[p] for p in prediction] for prediction in gen_labels
            ]
            labels = [[label_list[p] for p in prediction] for prediction in gt_labels]

            for preds, gts in zip(predictions, labels):
                n_pred = self._count_entities(preds)
                n_gt = self._count_entities(gts)
                if n_gt != 0:
                    proj_ratio_acc += n_pred / n_gt
                elif n_pred == 0:  # both 0, perfect projection
                    proj_ratio_acc += 1

            metric.add_batch(references=labels, predictions=predictions)

            if r_end == N_rows:
                break
            n_it += 1

        metrics_results = metric.compute()
        metrics_results["proj_ratio"] = proj_ratio_acc / len(ds_gen)

        print(metrics_results)

        if self._log_mflow:
            self._log_to_mlflow(metrics_results)
